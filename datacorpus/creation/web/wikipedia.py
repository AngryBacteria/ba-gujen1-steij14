import csv
import os
from urllib.parse import quote_plus

import requests
import re
from bs4 import BeautifulSoup, ResultSet
from dotenv import load_dotenv
from pymongo import MongoClient

from datacorpus.utils.scraping import (
    process_tags_to_text,
    remove_unwanted,
)
from shared.logger import logger

article_ignore_list = [
    "Kategorie:Liste (Krankheiten nach ICD-10)",
    "Kategorie:Liste (Krankheitsopfer)",
    "Krankheitsbild in der Tiermedizin",
    "Krankheitsbild in der Wehrmedizin",
    "Kategorie:Krankheitsbild in der Tiermedizin",
    "Kategorie:Hypovitaminose",
]

validation_articles = [
    "https://de.wikipedia.org/wiki/Ariboflavinose",
    "https://de.wikipedia.org/wiki/Kopfschmerz",
]

relevant_categories = [
    "Krankheit",
    "Medizinische_Fachsprache",
    "Therapie",
    "Diagnostik",
    "Medizinische_Dokumentation",
]


# WIKI PHP API
def fetch_all_category_members(category: str):
    """Get all ids/titles of all members for a wikipedia category, including their page IDs.
    Category should be the name of the category without the "Kategorie:" prefix."""

    members = set()
    last_continue = {}
    encoded_category = category.replace(" ", "_")
    while True:
        params = {
            "action": "query",
            "format": "json",
            "list": "categorymembers",
            "cmtitle": f"Kategorie:{encoded_category}",
            "cmtype": "page|subcat",
            "cmlimit": 500,
            **last_continue,
        }

        response = requests.get(url="https://de.wikipedia.org/w/api.php", params=params)
        if not response.ok:
            logger.error(
                f"Request failed with status code ({response.status_code}) for url: {response.url}"
            )
            continue

        logger.debug(f"Getting category members from: {response.url}")
        data = response.json()

        for item in data["query"]["categorymembers"]:
            members.add((item["title"], item["pageid"]))
        if "continue" not in data:
            break
        else:
            last_continue = data["continue"]

    return members


def get_all_titles_in_category(category):
    """Get all article ids/titles in a wikipedia category, including subcategories. Each article is represented
    by a tuple containing the title and page ID.
    Category should be the name of the category without the "Kategorie:" prefix."""

    articles = set()
    members = fetch_all_category_members(category)

    for title, pageid in members:
        if title in article_ignore_list:
            continue
        if title.startswith("Kategorie:"):
            logger.debug(f"Parsing subcategory: {title}")
            subcategory = title.split("Kategorie:")[1]
            articles.update(get_all_titles_in_category(subcategory))
        else:
            articles.add((title, pageid))

    return articles


def get_articles_views(title: str):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    encoded_title = quote_plus(title.replace(" ", "_"))
    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/de.wikipedia/all-access/all-agents/{encoded_title}/monthly/2015100100/2024030100"
    response = requests.get(url, headers=headers)
    if not response.ok:
        logger.error(
            f"Request failed with status code ({response.status_code}) for url: {response.url}"
        )
        return None
    else:
        output = 0
        data = response.json()
        for entry in data["items"]:
            output += entry["views"]
        return output


def save_article_ids(categories: list[str]):
    """Save the articles of multiple categories into a single file.
    The file will be named 'wikipedia_article_titles.txt'.
    The data is saved as a CSV in the format: Title|Page ID|Category"""
    all_articles = set()
    for category in categories:
        output = get_all_titles_in_category(category)
        for article in output:
            all_articles.add((article[0], article[1], category))

    file_path = "wikipedia_article_titles.txt"
    with open(file_path, mode="w", encoding="utf-8") as file:
        writer = csv.writer(file, delimiter="|")
        writer.writerow(["Title", "Page ID", "Category"])
        for article in all_articles:
            writer.writerow(article)

    logger.debug(
        f"Saved {len(all_articles)} articles from the categories into: {file_path}"
    )
    return all_articles


def read_article_ids_from_file():
    """Read the articles and their categories from the file generated by the save_article_ids function."""
    file_path = "wikipedia_article_titles.txt"
    articles = set()
    with open(file_path, mode="r", encoding="utf-8") as file:
        reader = csv.reader(file, delimiter="|")
        next(reader)
        for row in reader:
            articles.add((row[0], row[1], row[2]))
    logger.debug(f"Read {len(articles)} articles from: {file_path}")
    return articles


# WEB SCRAPING
def clean_wikipedia_string(text: str):
    """Clean a string from wikipedia by removing references and other unwanted elements."""
    text = text.strip()
    pattern = r"\[.*?\]"
    text = re.sub(pattern, "", text)
    return text


def get_wikipedia_article_data(title: str, get_full_text: bool = True):
    """Get the data of a wikipedia article by title.
    If get_full_text is set to True, the full text of the article is returned.
    If get_icd10 is set to True, the ICD-10 codes are also returned."""
    # make request
    encoded_title = quote_plus(title.replace(" ", "_"))
    link = (
        f"https://de.wikipedia.org/api/rest_v1/page/html/{encoded_title}?redirect=false"
    )
    response = requests.get(link, allow_redirects=False)
    if not response.ok:
        logger.error(
            f"Request failed with status code ({response.status_code}) for url: {response.url}"
        )
        return None
    soup = BeautifulSoup(response.content, "html.parser")

    # get icd10 codes and return data
    icd10_info_table = soup.find_all("div", class_="float-right")
    if icd10_info_table is None or len(icd10_info_table) == 0:
        codes_who = []
        codes_gm = []
    else:
        codes_who, codes_gm = parse_icd10_table(icd10_info_table)

    # remove unwanted tags and get content div
    soup = remove_unwanted(soup)
    content_div = soup.find("body", class_=["mw-content-ltr", "mw-parser-output"])
    if content_div is None:
        logger.warning(f"No content div found for: {title}")
        return None

    # get the introduction text
    sections = content_div.find_all(name="section", recursive=False)
    introduction_text = get_all_wiki_text(sections, False)
    if introduction_text == "":
        logger.warning(f"No introduction text found for: {title}")
        return None

    # get the full text
    if get_full_text:
        sections = content_div.find_all(name="section", recursive=False)
        full_text = get_all_wiki_text(sections, True)
        if full_text == "":
            logger.warning(f"No full text found for: {title}")
            return None
    else:
        full_text = ""

    return {
        "title": title,
        "text": clean_wikipedia_string(introduction_text),
        "full_text": clean_wikipedia_string(full_text),
        "icd10_who": codes_who,
        "icd10_gm": codes_gm,
    }


def get_all_wiki_text(sections: ResultSet, full_text: bool) -> str:
    """Get text for a wikipedia article from the content div.
    If full_text is set to True, the full text of the article is returned.
    Otherwise, only the introduction text is returned."""
    text = ""
    if not full_text:
        sections = [sections[0]]

    for section in sections:
        tags = section.find_all(
            ["h1", "h2", "h3", "h4", "h5", "h6", "p", "ul", "ol", "dl"]
        )
        text = text + process_tags_to_text(tags)

    if text.startswith("* Wikidata:"):
        return ""
    return text


def parse_icd10_table(icd10_infos: ResultSet):
    """Parse the ICD-10 table data from a wikipedia article. The infobox component is used."""
    output_icd10_who = []
    output_icd10_gm = []
    for icd10_info in icd10_infos:
        table = icd10_info.find("table")
        if table is None:
            continue
        rows = table.find_all("tr")
        if rows is None or len(rows) == 0:
            continue

        # check if icd10-who is used for article
        if rows[0] is not None:
            title = rows[0].text.replace("\n", " ").strip()
            if title == "Klassifikation nach ICD-10":
                for row in rows:
                    row_text = row.text.replace("\n", " ").strip()
                    pattern = r"\b([A-Z][0-9]{2}(?:\.[0-9]{1,2})?[\+\*]?)\b"
                    icd10_codes = re.findall(pattern, row_text)
                    if icd10_codes is not None and len(icd10_codes) > 0:
                        output_icd10_who.extend(icd10_codes)

            elif title == "Klassifikation nach ICD-10-GM":
                for row in rows:
                    row_text = row.text.replace("\n", " ").strip()
                    pattern = r"\b([A-Z][0-9]{2}(?:\.[0-9]{1,2})?[\+\*]?)\b"
                    icd10_codes = re.findall(pattern, row_text)
                    if icd10_codes is not None and len(icd10_codes) > 0:
                        output_icd10_gm.extend(icd10_codes)

    return output_icd10_who, output_icd10_gm


def add_views_to_db(overwrite=False):
    """Add the views of the articles to the articles already present in the MongoDB database."""
    load_dotenv()
    client = MongoClient(os.getenv("MONGO_URL"))
    db = client.get_database("web")
    wikipedia_icd10_collection = db.get_collection("wikipedia_icd10")
    for doc in wikipedia_icd10_collection.find():
        try:
            if "views" in doc and not overwrite:
                continue
            title = doc["title"]
            views = get_articles_views(title)
            if views is not None:
                wikipedia_icd10_collection.update_one(
                    {"title": title}, {"$set": {"views": views}}
                )
                logger.debug(f"Updated Article ({title}) with {views} views.")
        except Exception as e:
            logger.error(f"Error for when getting views for article: {e}")
    client.close()


def build_wikipedia_db(categories: list[str], from_file=True):
    """Build a MongoDB collection with the articles from a wikipedia category."""
    load_dotenv()
    client = MongoClient(os.getenv("MONGO_URL"))
    db = client.get_database("web")
    wikipedia_collection = db.get_collection("wikipedia")
    wikipedia_collection.create_index("title", unique=True)

    if not from_file:
        save_article_ids(categories)
    articles = read_article_ids_from_file()

    for title, _id, category in articles:
        try:
            # check if article already exists to avoid duplicates
            existing_article = wikipedia_collection.find_one({"title": title})
            if existing_article is None:
                data = get_wikipedia_article_data(title, True)
                if data is not None:
                    data["category"] = category
                    data["pageid"] = _id
                    wikipedia_collection.insert_one(data)
                    logger.debug(f"Uploaded {title}({_id}) to MongoDB.")
            else:
                logger.debug(f"Article {title}({_id}) already exists in MongoDB.")
        except Exception as e:
            logger.error(f"Failed to upload {title}({_id}): {e}")
    client.close()


if __name__ == "__main__":
    build_wikipedia_db(relevant_categories, from_file=True)
